{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVeMa 2018\n",
    "\n",
    "![logo](assets/logo.jpg \"Logo\")\n",
    "\n",
    "- Instructor: Juan Esquivel.\n",
    "\n",
    "- Authors: \n",
    "    - Saúl Calderón, Martín Solís, Ángel García, Blaž Meden, Žiga Emeršič, Felipe Meza, Juan Esquivel\n",
    "    - Mauro Méndez, Manuel Zumbado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "This workshop has the main objective of giving attendees a basic understanding of the different aspects involved in the day-to-day tasks performed by the data scientist role. Note that the definition of what Data Science actually is can be vague. However, some of the core competencies that need to be taught are:\n",
    "\n",
    "- **Reading and transforming data at large scale**. This falls under the standard ETL philosophy where data needs to be *Extracted* from a source, it needs to be *Transformed* either to create more complex data or to adjust to the target destination and, finally, data needs to be *Loaded* into a medium where it will be consumed by users. Over the last decade the need to handle the ETL process at scale has become key to the effectiveness of a data scientist. In this workshop we will use Apache Spark as a way to introduce participants into this type of processing.\n",
    "\n",
    "- **Data Analytics**. A broad term that covers applying techniques that allow us to make the data more compact and thus interpretable. The methodology behind the analyses can range from very ad hoc analysis, such as running SQL queries and averaging columns of interest, to formal statistical tests to compare multiple sets of data to determine similarity, for instance. What framework the analytics takes place in can vary wildly depending on the data stack that a particular organization uses. However, statistical frameworks like `R` are common. Python tends to be the language of choice for science-based projects and it has matured into a very robust option, thanks to the vast number of libraries available.\n",
    "\n",
    "- **Learning**. A data scientist will not necessarily develop new machine learning algorithms but he/she should be able to apply them to specific problems that are faced. For instance, by doing analytics we could identify purchasing patterns by customers at a particular store. Are these patterns systematic enough that we can predict, given products already purchased, what other products that client might be interested in? At what price range? Would the client need an offer to decide to purchase the product? All of these are questions that could be answered by learning a model. Scikit learn is the de facto library in Python. Frameworks like Tensorflow and Keras (which can wrap Tensorflow) are also available as APIs in Python.\n",
    "\n",
    "- **Experiment design and execution**. Related to the *Science* part in *Data Science*, it is not uncommon for a person in this role to think about validating the trends and conjectures made from the analysis in order to create verifiable knowledge. For example, if a social media website release a change in their UI and see a 20% increase in engagement the 5 days after the release, is it due to the UI changes or was there some social event affecting a large population that coincided with that release? How do we know for sure that it was the product change? This is where designing experiments comes in and it goes from defining target populations, sampling strategies to formal statistical testing.\n",
    "\n",
    "- **Visualization**. It could be considered more intimately related to analytics, however, visualization is ubiquitous in everything a data scientist does. The person will not necessarily develop novel visualizations but should be aware of effective ways of showing the data in aggregate fashion to the consumers. It can range from relatively simple plotting with libraries like `matplotlib` to potentially very expensive cloud packages, like *Tableau*.\n",
    "\n",
    "# Data processing at large scale\n",
    "Nowadays it is relatively simple to have vast amounts of organizational data without even realizing it. Both local and cloud storage services give us the ability to store a significant amount of information about customers, transactions, projects, and so on. Without even realizing it, after a couple of years, an analysis program that *used to work* can stop working just because of the organic data growth.\n",
    "\n",
    "As mentioned before, one of the core competencies of a data scientist is to be able to process large amounts of data. This is commonly done using high scalability frameworks inspired by a programming model called *MapReduce*. In essence, the idea is to think about data as a large number of single records and we want to apply specific operations to each of them, independent of any other record (at least in theory). These operations that modify the records are called *map* functions. In addition to mapping an input to a transformed output we can sythesize multiple records that correspond to the same entity by *reducing* them from multiple records into one. While current frameworks do not necessarily hold to the exact spirit of *MapReduce* the ideas are the same.\n",
    "\n",
    "To serve as an example we will assume there is an online retailer that stores customer purchases in a PostgreSQL database and we will apply some basic operations on it to illustrate what these types of frameworks can do. A sample creation script for such a table, with a handful of transaction examples is given below:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE transactions (\n",
    "  id SERIAL PRIMARY KEY,\n",
    "  customer_id integer NOT NULL,\n",
    "  amount integer NOT NULL,\n",
    "  purchased_at timestamp without time zone NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "INSERT INTO \"transactions\" (customer_id, amount, purchased_at) VALUES\n",
    "(1, 55, '2017-03-01 09:00:00'),\n",
    "(1, 125, '2017-03-01 10:00:00'),\n",
    "(1, 32, '2017-03-02 13:00:00'),\n",
    "(1, 64, '2017-03-02 15:00:00'),\n",
    "(1, 128, '2017-03-03 10:00:00'),\n",
    "(2, 333, '2017-03-01 09:00:00'),\n",
    "(2, 334, '2017-03-01 09:01:00'),\n",
    "(2, 333, '2017-03-01 09:02:00'),\n",
    "(2, 11, '2017-03-03 20:00:00'),\n",
    "(2, 44, '2017-03-03 20:15:00');\n",
    "```\n",
    "\n",
    "## Apache Spark and distributed processing\n",
    "\n",
    "There are multiple frameworks that can be used for large scale data processing. For the purposes of this workshop we will use *Apache Spark* because it has widespread adoption, it is an open project and it also has cloud support from commercial cloud vendors (like Amazon Web Services).\n",
    "\n",
    "Spark is based on an abstraction called *Resilient Distributed Dataset* which is a collection of elements that can be stored (temporarily or permanently) across multiple nodes in a cluster. Spark supports multiple sources of information, from SQL and NoSQL driven databases, to simple text files. Transformation operations are applied on RDDs and a programmer's main job centers around designing a series of operations that need to be applied to a source dataset in order to transform it into the final output desired.\n",
    "\n",
    "The following snippet shows a basic example to read a transactions table from a database and display its contents. Note that if the table were big, calling the `show()` method is unwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+\n",
      "| id|customer_id|amount|       purchased_at|\n",
      "+---+-----------+------+-------------------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00|\n",
      "|  2|          1|   125|2017-03-01 10:00:00|\n",
      "|  3|          1|    32|2017-03-02 13:00:00|\n",
      "|  4|          1|    64|2017-03-02 15:00:00|\n",
      "|  5|          1|   128|2017-03-03 10:00:00|\n",
      "|  6|          2|   333|2017-03-01 09:00:00|\n",
      "|  7|          2|   334|2017-03-01 09:01:00|\n",
      "|  8|          2|   333|2017-03-01 09:02:00|\n",
      "|  9|          2|    11|2017-03-03 20:00:00|\n",
      "| 10|          2|    44|2017-03-03 20:15:00|\n",
      "+---+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark-2.2.0-bin-hadoop2.7')\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, udf \n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Basic JDBC pipeline\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"postgresql-42.1.4.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"postgresql-42.1.4.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reading single DataFrame in Spark by retrieving all rows from a DB table.\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost/evema\") \\\n",
    "    .option(\"user\", \"test\") \\\n",
    "    .option(\"password\", \"test\") \\\n",
    "    .option(\"dbtable\", \"transactions\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading raw data is rarely the ultimate objective. The two standard high-level tasks that are sought are: transforming some of the columns into modified representations of them or aggregating groups of rows into one. To illustrate this we will assume that we want to total the purchases made by clients, by date.\n",
    "\n",
    "Spark already provides a series of functions that can be applied to columns. To start the process to agreggate by date we will transform them into a formatted representation that drops the undesired parts of the date structure (note this is not the only way to do this, but it is useful as an example).\n",
    "\n",
    "For this purpose, we can use the `date_format` function in the `pyspark.sql.functions` module. The following snippet creates a new column with such transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+-----------+\n",
      "| id|customer_id|amount|       purchased_at|date_string|\n",
      "+---+-----------+------+-------------------+-----------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00| 03/01/2017|\n",
      "|  2|          1|   125|2017-03-01 10:00:00| 03/01/2017|\n",
      "|  3|          1|    32|2017-03-02 13:00:00| 03/02/2017|\n",
      "|  4|          1|    64|2017-03-02 15:00:00| 03/02/2017|\n",
      "|  5|          1|   128|2017-03-03 10:00:00| 03/03/2017|\n",
      "|  6|          2|   333|2017-03-01 09:00:00| 03/01/2017|\n",
      "|  7|          2|   334|2017-03-01 09:01:00| 03/01/2017|\n",
      "|  8|          2|   333|2017-03-01 09:02:00| 03/01/2017|\n",
      "|  9|          2|    11|2017-03-03 20:00:00| 03/03/2017|\n",
      "| 10|          2|    44|2017-03-03 20:15:00| 03/03/2017|\n",
      "+---+-----------+------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_df = df.withColumn(\"date_string\", date_format(col(\"purchased_at\"), 'MM/dd/yyyy'))\n",
    "formatted_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is natural in any programming endeavor, what happens if we need to create a function that is **not** coming from a Spark library to manipulate a column? The answer is a `udf` (User Defined Function). The basic notion of a `udf` in Spark is a function definition that also provides the Spark type that the lambda returns, which is necessary in a weakly-typed language like Python.\n",
    "\n",
    "This is also a nice way to wrap Python functions that need to be applied to cells, but do not have a column-wide library function already implemented in Spark.\n",
    "\n",
    "The following example transforms the previously created `date_string` column (which is of `string` type) into a column with the proper Spark `DateType`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+-----------+----------+\n",
      "| id|customer_id|amount|       purchased_at|date_string|      date|\n",
      "+---+-----------+------+-------------------+-----------+----------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00| 03/01/2017|2017-03-01|\n",
      "|  2|          1|   125|2017-03-01 10:00:00| 03/01/2017|2017-03-01|\n",
      "|  3|          1|    32|2017-03-02 13:00:00| 03/02/2017|2017-03-02|\n",
      "|  4|          1|    64|2017-03-02 15:00:00| 03/02/2017|2017-03-02|\n",
      "|  5|          1|   128|2017-03-03 10:00:00| 03/03/2017|2017-03-03|\n",
      "|  6|          2|   333|2017-03-01 09:00:00| 03/01/2017|2017-03-01|\n",
      "|  7|          2|   334|2017-03-01 09:01:00| 03/01/2017|2017-03-01|\n",
      "|  8|          2|   333|2017-03-01 09:02:00| 03/01/2017|2017-03-01|\n",
      "|  9|          2|    11|2017-03-03 20:00:00| 03/03/2017|2017-03-03|\n",
      "| 10|          2|    44|2017-03-03 20:15:00| 03/03/2017|2017-03-03|\n",
      "+---+-----------+------+-------------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string_to_date = \\\n",
    "    udf(lambda text_date: datetime.strptime(text_date,\n",
    "        '%m/%d/%Y'), DateType())\n",
    "\n",
    "typed_df = formatted_df.withColumn(\"date\", string_to_date(formatted_df.date_string))\n",
    "typed_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aggregate data we can use what should be relatively natural for people who use SQL. In Spark this can be achieved by combining a `groupBy` operation with an aggregation function. In this case, we want to sum all the purchases per customer and date, which can be achieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+----------------+-----------+\n",
      "|customer_id|      date|sum(id)|sum(customer_id)|sum(amount)|\n",
      "+-----------+----------+-------+----------------+-----------+\n",
      "|          2|2017-03-01|     21|               6|       1000|\n",
      "|          1|2017-03-02|      7|               2|         96|\n",
      "|          1|2017-03-03|      5|               1|        128|\n",
      "|          1|2017-03-01|      3|               2|        180|\n",
      "|          2|2017-03-03|     19|               4|         55|\n",
      "+-----------+----------+-------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_df = typed_df.groupBy(\"customer_id\", \"date\").sum()\n",
    "sum_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how Spark will aggregate **every other column** not in the `groupBy` operation, which can lead to confusion. For instance, aggregating all row identifiers is meaningless. To clean up the data we can extract the columns we are interested in. Also, we can give column aliases to improve redability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = false)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      "\n",
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|          2|2017-03-01|  1000|\n",
      "|          1|2017-03-02|    96|\n",
      "|          1|2017-03-03|   128|\n",
      "|          1|2017-03-01|   180|\n",
      "|          2|2017-03-03|    55|\n",
      "+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats_df = \\\n",
    "    sum_df.select(\n",
    "        col('customer_id'),\n",
    "        col('date'),\n",
    "        col('sum(amount)').alias('amount'))\n",
    "\n",
    "stats_df.printSchema()\n",
    "stats_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that without any bells and whistles we just used a very powerful operation in Spark: `select`. The breadth of features that `select` offers is out of scope here, but attendees are encouraged to review the documentation on the operation to learn from its very flexible implementation.\n",
    "\n",
    "So far we have used a SQL source because relational databases are very much the primary source of data for organizations (after a brief NoSQL-dominated hiatus a few years ago). However, it is expected that some of the data lives outside of a database management system. For instance, flat text files are more prevalent than one would imagine.\n",
    "\n",
    "Let us assume that, to protect user privacy, the actual customer names are not stored in the database. Because we want to exemplify what we could do with text files, we can have them stored on unencrypted, human-readable CSV files (which is obviously secure!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+---+----+--------+\n",
      "| id|name|currency|\n",
      "+---+----+--------+\n",
      "|  1|John|     CRC|\n",
      "|  2|Jane|     EUR|\n",
      "+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "\n",
    "names_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"names.csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(StructType([\n",
    "                StructField(\"id\", IntegerType()),\n",
    "                StructField(\"name\", StringType()),\n",
    "                StructField(\"currency\", StringType())])) \\\n",
    "    .load()\n",
    "\n",
    "names_df.printSchema()\n",
    "names_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because CSV files do not have explicit type information (as opposed to a relational database) it is good practice to explicitly provide a schema, although it is not required and can be inferred.\n",
    "\n",
    "Once a data source has been loaded onto a Spark dataframe, where it came from becomes irrelevant and we can mix them together as needed. If we wanted to augment our dataframe with transaction summary data we could **JOIN** the names dataframe and the aggregates one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = false)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+-----------+----------+------+---+----+--------+\n",
      "|customer_id|      date|amount| id|name|currency|\n",
      "+-----------+----------+------+---+----+--------+\n",
      "|          2|2017-03-01|  1000|  2|Jane|     EUR|\n",
      "|          1|2017-03-02|    96|  1|John|     CRC|\n",
      "|          1|2017-03-03|   128|  1|John|     CRC|\n",
      "|          1|2017-03-01|   180|  1|John|     CRC|\n",
      "|          2|2017-03-03|    55|  2|Jane|     EUR|\n",
      "+-----------+----------+------+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joint_df = stats_df.join(names_df, stats_df.customer_id == names_df.id)\n",
    "joint_df.printSchema()\n",
    "joint_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise for attendees:\n",
    "## If we assume the amounts in the database are US dollars, load exchange_rates.csv which\n",
    "## contains the exchange rate for countries that match customer country codes. Create and show a dataframe where\n",
    "## the amounts are in the customer's local currency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few Spark technical notes\n",
    "Up to this point we have discussed, intuitively, some of the principles behind Spark. This is not meant to be a Spark dissertation but some nuance about how and why Spark operates the way it does is worth highlighting.\n",
    "\n",
    "- **Collecting results**. As mentioned before, Spark is based on the concept of a distributed dataset that could live completely in a single machine or distributed across many. An operation called `collect` allows the programmer to merge all results together on a single machine. This is what happens behind the scenes whenever we do `show()` on each of the snippets before. This operation should be used with caution (or the framework could crash if large amounts of data are collected in one place).\n",
    "- **Graph construction and evaluation**. If we remove all `show()` statements from the code so far what we would have done is tell Spark what sequence of operations it will eventually perform, but it hasn't really processed any of the data. Spark is what we call a *lazy evaluator* because it does not evaluate operations until it really has to, in order to comply with the programmer explicit result expectations. What it *does* do is create a graph that represents the data flow going from the sources to the final dataframes created in the code. Whenever a `collect`-based operation is triggered, Spark will retrieve the deepest node needed to comply with the request, it evaluates all paths in the graph needed to create the results in that node, and then it will run the code configured.\n",
    "- **Persistence**. Where do we store results? The answer is, most likely, distributed files, where slices of the data will be stored in one machine and other slices in another. However, we can also use Spark to write results into a relational database. The limitation is that we can only create a complete table to store all results that match a dataframe, or simply append a series of rows to a table with matching schema. If we wanted to have update operations we need to implement our own code layer to handle this. Two common large-scale reasons why we would store Spark results are archiving and data warehousing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics\n",
    "Another core competency of a data scientist is to be able to understand what the data are and any patterns that arise from it. While the tasks related to large scale processing are closer to software engineering, the ones on the analytics side fall closer to the field of statistics.\n",
    "\n",
    "There is a pre-conception that analytics are based on metrics that are very hard to derive. Most people with some background in Mathematics can speak the language of *sums* and *averages*, which are surprisingly effective sometimes. Having said that, basic statistics should only be considered a first level of analysis and we are expected to dig deeper into the data to find insight that is less obvious to the eye.\n",
    "\n",
    "See the following example where can imagine that we mimic monthly salary ranges for a population. The first example shows a society where there is a total of 550 available in the economy, but the low income people earn one order of magnitude less than the wealthy people. In the second, people earn roughly the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "L08kjoIuBQdD",
    "outputId": "52ba6383-51d3-4e3b-fa99-8c11466507cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unequal average: 55.0\n",
      "Unequal median: 55.0\n",
      "Unequal sum: 605\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7edc25668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "unequal_income = np.array([10, 10, 10, 10, 10, 55, 100, 100, 100, 100, 100])\n",
    "\n",
    "print('Unequal average: {}'.format(np.average(unequal_income)))\n",
    "print('Unequal median: {}'.format(np.median(unequal_income)))\n",
    "print('Unequal sum: {}'.format(np.sum(unequal_income)))\n",
    "\n",
    "_ = plt.hist(unequal_income, range=(0, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better average: 55.0\n",
      "Better median: 55.0\n",
      "Better sum: 550\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7fc36cac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "better_income = np.array([50, 50, 50, 50, 50, 60, 60, 60, 60, 60])\n",
    "\n",
    "print('Better average: {}'.format(np.average(better_income)))\n",
    "print('Better median: {}'.format(np.median(better_income)))\n",
    "print('Better sum: {}'.format(np.sum(better_income)))\n",
    "\n",
    "_ = plt.hist(better_income, range=(0, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving away from point estimates\n",
    "One first step towards better analysis is to incorporate the *spread* of the data into the analysis. This can be done with two metrics that are still familiar to people with some mathematical background but, for some reason, we tend to use them less: standard deviation and variance.\n",
    "\n",
    "While the previous example shows two datasets that cannot be differentiated by the metrics chosen, by adding standard deviation and/or variance the nuances between both become clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unequal standard dev: 42.90581651605166\n",
      "Unequal variance: 1840.909090909091\n",
      "Better standard dev: 5.0\n",
      "Better variance: 25.0\n"
     ]
    }
   ],
   "source": [
    "print('Unequal standard dev: {}'.format(np.std(unequal_income)))\n",
    "print('Unequal variance: {}'.format(np.var(unequal_income)))\n",
    "\n",
    "print('Better standard dev: {}'.format(np.std(better_income)))\n",
    "print('Better variance: {}'.format(np.var(better_income)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using metrics related to distribution spread is a clear step forward, but we still have work to do. For instance, how do we know if the difference between two standard deviations is *large enough*? For starters the comparison will be very scale dependent and subtle differences might be very significant in some contexts, while not so much in others (note that we have not talked about normalization of the data).\n",
    "\n",
    "Everything in data analysis can be seen as layers that need to be explored further. From this type of metric a good rule of thumb is to analyze the change in order(s) of magnitude from one metric to the other. Note that in the sample case both the standard deviation and variance have 1 and 2 orders of magnitude changes between each other. It should be pretty non controversial that under those circumstances there is something going on.\n",
    "\n",
    "## Distributions are your friends, but first...\n",
    "If using standard deviation and variance is not enough, then what is? In an ideal world we would think about every single column that describes our problem of interest as a distribution. We would abandon the notion of using point estimates and whenever we want to *compare two columns* we would do hypothesis testing over both of them (more on this later). There are several practical road blocks to being able to do this so we need some basic tools that allow us to avoid going into full *the world is just distributions* mode, while still getting us close enough.\n",
    "\n",
    "Thus, before distributions, we also like to look at **quantiles**. The two most famous divisions are **quartiles** and **percentiles** but, technically, any uniform set of buckets that divide the data are considered quantiles. To illustrate this concept, let us draw a hypothetical distribution of male individuals and their heights from two different countries on Earth (based on the always reliable [Wikipedia](https://en.wikipedia.org/wiki/List_of_average_human_height_worldwide))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dutch quartiles => Q1: 177.98789658757937 Q2: 184.80290932715246 Q3: 191.25407105159604\n",
      "Bolivian quartiles => Q1: 153.92730130021673 Q2: 160.40096482418767 Q3: 167.28616529017975\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7d6a542b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dutch = np.random.normal(184, 10, 1000)\n",
    "bolivian = np.random.normal(160, 10, 1000)\n",
    "\n",
    "print('Dutch quartiles => Q1: {} Q2: {} Q3: {}'.format(\n",
    "      np.percentile(dutch, 25),\n",
    "      np.percentile(dutch, 50),\n",
    "      np.percentile(dutch, 75)))\n",
    "\n",
    "print('Bolivian quartiles => Q1: {} Q2: {} Q3: {}'.format(\n",
    "      np.percentile(bolivian, 25),\n",
    "      np.percentile(bolivian, 50),\n",
    "      np.percentile(bolivian, 75)))\n",
    "\n",
    "plt.hist(dutch, bins=10, alpha=0.5, label='dutch')\n",
    "plt.hist(bolivian, bins=10, alpha=0.5, label='bolivian')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of countries was made in order to have a clear example and it may be apparent that the 2 simulated populations are different. However, if we always used deciles, for example, we would be comparing 9 different values against other 9, which is much better than comparing single point averages.\n",
    "\n",
    "As we mentioned before, the best approach we can take here is to analyze full distributions and compare whether or not they are significantly different.\n",
    "\n",
    "## Hypothesis testing\n",
    "Every column in a dataset follows some distribution or, at least, comes from some distribution that we may or may not know. The final and most appropriate way of reasoning about columns is determining what distribution originated them and then do any form of comparison and analysis based on the properties of the distribution.\n",
    "\n",
    "Before, we assumed that heights are normal because many measures in nature are. However, it is common not to have enough domain knowledge about a variable to intuitively know what distribution originated it. Two common tools to do this are normality tests or a KS-test (as in Kolmogorov-Smirnov). The former, as its name suggests, checks whether a sample follows a Gaussian distribution while the second one is a more general goodness-of-fit test. We will not dive into the statistical details but it is important that we know that they exist as tools.\n",
    "\n",
    "For example, using `scipy.stats.normaltest` we can corroborate that the two height samples we generated are indeed normal. The null hypothesis here is that the sample does follow a normal distribution so we obtain p-values that are large (not close to zero), which requires us to \"accept the null\". If we were to obtain very small p-values it means that the distributions are not normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bolivian normal p-value: 0.5022576247628789 Dutch normal p-value: 0.9818220040397851\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as sps\n",
    "\n",
    "bolivian_normal_test = sps.normaltest(bolivian)\n",
    "dutch_normal_test = sps.normaltest(dutch)\n",
    "\n",
    "print('Bolivian normal p-value: {} Dutch normal p-value: {}'.format(\n",
    "    bolivian_normal_test.pvalue, dutch_normal_test.pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common operation is to determine whether two samples are the same. In practical terms this mean testing whether or not the means of the two distributions are centered around the same values. There are multiple statistical tests that can be applied but arguably a data scientist's best friend for this task is the t-test.\n",
    "\n",
    "The following snippet tries Welch's t-test on the Dutch and Bolivian populations generated before. Barring an unlikely sample created by `numpy.random` the p-values returned should be very close to zero, which means that the null hypothesis that the 2 distributions have identical averages must be rejected (i.e. they are not the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welch's test p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "# False makes it Welch's which doesn't assume same variance or size.\n",
    "test_result = sps.ttest_ind(dutch, bolivian, equal_var=False)\n",
    "print('Welch\\'s test p-value: {}'.format(test_result.pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise for attendees:\n",
    "# Select 2 countries from the Wikipedia list that you think are similar and follow the same procedure\n",
    "#   - Create a sample assuming normality for each\n",
    "#   - Since wikipedia does not provide standard deviation use a value between 8-12 centimeters (which is common)\n",
    "#   - Do Welch's t-test\n",
    "\n",
    "# Regardless of country, for a standard deviation of 10 cm, determine how many centimeters apart 2 sampled populations\n",
    "# need to be in order to get consistent p-values that suggest similarity (i.e. large p-values, not close to zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning in action\n",
    "Following the practical theme of this workshop, in this section we will assume that people have some notion as to what learning is and we will try to address some rules of thumb and recommendations to solve practical problems using ML.\n",
    "\n",
    "## Selecting a model family\n",
    "Learning has 3 main branches. We will briefly talk about each choosing a real life example to highlight what it is and when a Data Scientist would try to use such a model in their day to day work.\n",
    "\n",
    "### Unsupervised learning\n",
    "\n",
    "If we have thousands of family pictures that we want to classify but we have no idea where to start then we first need to group data in some coherent fashion. For instance, we may want to group photos where the same family member appears or by rough location (e.g. at the beach). As most people with smart phones know you never get to classify your pictures because there is too many of them.\n",
    "\n",
    "Wouldn't it be nice to have someone group them for you. That is what unsupervised learning tries to do: without the **supervision** of a human, an algorithm will create groups that (hopefully) have some meaning. The canonical unsupervised learning technique is called **clustering**.\n",
    "\n",
    "In general, unsupervised learning is more explorational and we will use it when we are exploring a domain that we have no certainty about either what we want or what each example in a dataset are. This type of learning can be a precursor to supervised learning, since annotating groups of entities can be easier than annotating every single individual in a dataset.\n",
    "\n",
    "### Supervised learning\n",
    "\n",
    "Maybe I have some spare time to classify 100 of my pictures. Maybe 500? I can come up with interesting annotations for each picture, ranging from names, locations, situations, and so on.\n",
    "\n",
    "Supervised learning models can take over a task where a human (or some other entity) assigns some ground truth value to each of the examples in a dataset and then extrapolates what other unseen examples should be treated as. The presumption here is we can start from a well curated *training* dataset that we know everything for and later in time there will be more instances of the problem that we will know what to do about, because we learned from the training phase.\n",
    "\n",
    "Two broad types of supervised learning exist: predicting a value (such as the number of people in a picture) and predicting annotations (e.g. beach, office). Thus, we use supervised learning models when we want to estimate the salary paid to a particular type of job, or when we want to predict whether or not a customer of some profile type will buy a product or not.\n",
    "\n",
    "The main problem with using this type of algorithms in practice is having annotated data. There are companies that dedicate themselves to just annotating data that then gets fed into ML pipelines for model creation. To use supervised models we **must** have data and organizations must first solve this challenge before trying to get into applying ML.\n",
    "\n",
    "### Reinforcement learning\n",
    "\n",
    "We will not dwell on this one because, compared to the previous ones, it is much less mature. The essence of this type of learning is that an *artificial intelligence agent* can learn the general rules about a domain (normally called a policy) that allows it to provide answers, navigate a space, etc.\n",
    "\n",
    "In recent years there have been prominent advances in board-like games being played by agents trained with a combination of reinforcement learning and supervised learning. Most prominently [AlphaGo](https://deepmind.com/research/alphago), developed by Google's DeepMind, was able to become arguably the best player on the planet in the very complicated game of Go.\n",
    "\n",
    "## Applying a model\n",
    "Once we have identified our type of problem, and selected which of the 3 branches of learning can help us, we can build a model and use it. As mentioned before, the purpose of this workshop is to focus on the practical implications of each data science competency, and not to go into details about particular learning models. Because of that we will focus on supervised learning only, specifically classification problems, since they are probably the most mature subset.\n",
    "\n",
    "Python has been our weapon of choice in no small part due to the vast support for scientific computation and artifical intelligence libraries. One such package called `scikit-learn` focuses on learning, as its name suggests, and its built on top of `numpy` and `scipy` which allows for interoperation between libraries (`tensorflow` is also a good option in Python).\n",
    "\n",
    "The package contains a handful of datasets that can be used for simple examples and thus we will use one of them: the Iris dataset. Iris is a type of flower that can be described by the petal and sepal (the green base that holds the petals) length and width. Different measures define different type of Iris varieties. The dataset contains 3 types of Iris and the 4 features described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris features\n",
      "\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Total Samples: 600\n",
      "\n",
      "5 Iris examples\n",
      "\n",
      "Sepal length/width: 5.1/3.5 Petal length/width: 1.4/0.2 Iris Type: 0\n",
      "Sepal length/width: 5.4/3.4 Petal length/width: 1.7/0.2 Iris Type: 0\n",
      "Sepal length/width: 5.9/3.2 Petal length/width: 4.8/1.8 Iris Type: 1\n",
      "Sepal length/width: 5.5/2.6 Petal length/width: 4.4/1.2 Iris Type: 1\n",
      "Sepal length/width: 6.4/3.2 Petal length/width: 5.3/2.3 Iris Type: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "sample_data = datasets.load_iris()\n",
    "\n",
    "print('Iris features\\n')\n",
    "print(sample_data.feature_names)\n",
    "\n",
    "print('Total Samples: {}'.format(sample_data.data.size))\n",
    "\n",
    "print('\\n5 Iris examples\\n')\n",
    "\n",
    "# Use different indexes to show different types (they are sorted in the returned set)\n",
    "for i in [0, 20, 70, 90, 115]:\n",
    "    print('Sepal length/width: {}/{} Petal length/width: {}/{} Iris Type: {}'.format(\n",
    "            sample_data.data[i, 0],\n",
    "            sample_data.data[i, 1],\n",
    "            sample_data.data[i, 2],\n",
    "            sample_data.data[i, 3],\n",
    "            sample_data.target[i]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train a model to predict what type of Iris we have based on the petal and sepal dimensions. We will use both Logistic Regression and Decision Tree classifiers mostly to illustrate that, from the outside, using one or another Machine Learning model is pretty well abstracted by the library (and other libraries do a similarly good job).\n",
    "\n",
    "While we keep default options for each classifier in this example, being able to tune them is where the real value of a data scientist can lie. The interface may be similar, but the specific parameters for each model and the circumstances under which one performs better than the other can vary.\n",
    "\n",
    "The following snippet trains both models and then reconstructs what the predicted labels are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Predictions\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "\n",
      "Decision Tree Predictions\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "\n",
      "Logistic Regression Accuracy: 0.96\n",
      "\n",
      "Decision Tree Accuracy: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, tree\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(sample_data.data, sample_data.target)\n",
    "logreg_predictions = logreg.predict(sample_data.data)\n",
    "print('Logistic Regression Predictions\\n')\n",
    "print(logreg_predictions)\n",
    "\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(sample_data.data, sample_data.target)\n",
    "dt_predictions = dt.predict(sample_data.data)\n",
    "print('\\nDecision Tree Predictions\\n')\n",
    "print(dt_predictions)\n",
    "\n",
    "# Calculate accuracy\n",
    "logreg_correct = 0\n",
    "dt_correct = 0\n",
    "\n",
    "for i in range(0, sample_data.target.size):\n",
    "    if sample_data.target[i] == logreg_predictions[i]:\n",
    "        logreg_correct += 1\n",
    "    if sample_data.target[i] == dt_predictions[i]:\n",
    "        dt_correct += 1\n",
    "\n",
    "print('\\nLogistic Regression Accuracy: {}\\n'.format(logreg_correct / sample_data.target.size))\n",
    "print('Decision Tree Accuracy: {}\\n'.format(dt_correct / sample_data.target.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a model\n",
    "\n",
    "In the example we compute accuracy since it is a natural measure to determine whether or not the model was able to correctly classify examples. We calculated that value over the set of the exact same examples we used for fitting. While accuracy is not the best measure we could use (e.g. confusion matrices) evaluating the same training set is a big no no in learning.\n",
    "\n",
    "The main problem is that this value *should be good* and it will only reflect how well the model adjusted to the test data, but not how well it generalizes. To address this a better protocol for evaluation involves splitting the data in different ways.\n",
    "\n",
    "### Training and testing\n",
    "\n",
    "At minimum, we want to split the data in 2:\n",
    "- Training set: Some percentage of the data (possibly 50% at random) is fed into the fitting function and the classifier is saved.\n",
    "- Testing set: The rest of the data is fed into the predicting function. Since the fitting function never saw these data, and assuming the original data set was split in a way that both partitions are representative of the population, if we get good metrics then there is hope that the model is indeed good.\n",
    "\n",
    "### Training, validation and testing\n",
    "\n",
    "A second step in the right direction is to have 3 sets: training, validation and test. These can be even splits but ratios like 40-40-20 are common too. The main drawback with using just 2 datasets is that we will likely try to run the model, tweak it, run again, tweak, and so on. By doing this there is the risk that we are manually overfitting to the testing set. The third set accounts for this problem with one big restriction: it should ideally be used only once for a final evaluation of the model. The third set becomes a sort of acid test for the model.\n",
    "\n",
    "This also suggests that it is good practice not to tweak models excessively. Sound reasoning before creating models can trump obsessive back and forths between training and testing.\n",
    "\n",
    "To the very purist if the final test is not successful, and we do not want to throw the model away, then we need to find completely fresh data.\n",
    "\n",
    "### K-fold cross-validation\n",
    "A third and final protocol for evaluation is an extension of the second one and it is valled K-fold cross validation. We still have a third and final testing set that we will save until the end. However, instead of having just 2 sets for training and validation, we will make those two sets one (at least it seems like it) and then we will create K different training/validation pairs. If we have 1000 examples, we will divide them into say 10 groups (K=10) of 100 and we will train and test 10 different models. The first iteration we will use subgroups 1 through 9 for training and subgroup 10 for testing. In the second we may use subgroup 1 for testing and 2-10 for training.\n",
    "\n",
    "By doing this we try to compensate for the possibility that a particular selection of training and testing sets is causing some bias in the model.\n",
    "\n",
    "After we have finished all iterations the *final model* can be created by averaging the K runs. The very final acid test evaluation against the holdout third set would be done using that averaged model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise for attendees: use the load_wine dataset in sklearn.datasets but this time\n",
    "# use training, testing and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment design\n",
    "For some this is the best part of being a data scientist. You work in a particular domain, you analyze the data, you find patterns, you create models to synthesize data and automate the decision making process but that is arguably all based on past data.\n",
    "\n",
    "How do you generate new data? How do you test whether or not a change to your website will have the desired effect? For an online retailer, for instance, will adding a scale of 1-10 in reviews generate more or less sales than a 5 point scale?\n",
    "\n",
    "These questions fall under the competency of experiment design that should take into account how to define a population of interest, how to sample from it, how to perform the test and, finally, how to analyze it.\n",
    "\n",
    "## Defining population and sampling\n",
    "A natural way of relation to this task is to think about political polling. When an election approaches polling companies need to contact a small subset of the population and ask them who they will vote for. The best possible study one can make is a census, where the population and the sample become the same. Because the cost of a census is high, we want to be able to interview a (relatively) small subset of the whole population that will yield results that match the thinking of the whole population.\n",
    "\n",
    "A first approach to sampling is selecting completely at random. If we draw enough samples in theory the sample should be correctly distributed and will contain enough examples for all possible values, for all possible columns. In the case of election polling, we want to sample a number of people from each age range proportional to the total number of citizens in each of those ranges. We want to sample a number of low income and high income people that mimic the ratio in the whole country.\n",
    "\n",
    "Sampling is borderline a craft because it is rare that we can get a representative sample across the most important features just by randomly selecting examples.\n",
    "\n",
    "### Stratified sampling\n",
    "\n",
    "While there is debate among statistician about the best way to sample, one that combines practical considerations and shound theory is stratified sampling. It is basically a 2-step protocol to divide the population into a few categories (for some definition of *few*) that we want to have representation for and, within each category, we do apply a systematic way of sampling, possibly just truly random.\n",
    "\n",
    "The main difference between overall random and stratified lies in our willingness to accept a sample that does not have a single person between the ages of 18-25 in a sample, simply because a truly random sample allows for the possibility that no young people are selected. What stratified says is that we want to at least populate some important categories and after that everything is fair game.\n",
    "\n",
    "Socio-economic features are prevalent in election polling, but those are not the only person descriptors that can interest a data scientist. For instance, if we are doing user experience research on a website, features related to tech literacy are probably much more important than anything else. Thus we probably want to force some people to be complete computer geeks, others who are able technology users but struggle sometimes, but also allowing some people like the instructor's parents who can struggle knowing whether a photo was blatantly photo-shopped or if it is real.\n",
    "\n",
    "## A/B testing\n",
    "A/B testing refers to the notion that we can have two groups of users, patients, humans, and we want to determine what their reaction would be to some event or feature (A or B). In the case of 5 or 10 star reviews mentioned before, we want to expose half of them to the 5 point scale and half to the 10 point one. If the samples are good, based on the criteria defined in the previous section, we should get a good mix of individuals exposed to both treatments. Once we finish the experiment we could measure, for example, what's the average purchase of clients that see 5 versus the ones that see 10.\n",
    "\n",
    "Normally we should define a collection period, sit back and wait for the data. Conversely, we should **not** try to check on the partial results of the experiment every few hours. This will bias us or create preconceived notions that will not be beneficial for any analytics that are performed after the experiment. In particular we tend to be victims of confirmation bias, which is our tendency to give data an interpretation that is consistent with what we think, and not what the data truly says.\n",
    "\n",
    "### Traffic experiments\n",
    "A particular instance of this problem happens when we want sample requests to a Web server or website, in general. This is a good practical example of how to apply different treatments. If our website has a good enough amount of traffic, we can use tricks like using the session id assigned by the system modulo 100. We apply the A treatment to sessions between 0 and 49 and the B treatment to 50 to 99.\n",
    "\n",
    "We could presumably then sample uniformly and the data should be correctly distributed across the different descriptors that we care about.\n",
    "\n",
    "It is good practice for every system to have a form of experiment management layer. While we are running the 5 versus 10 star experiment, we may also want to run another one with a yellow \"Buy\" button versus a green one. There are more elaborate ways to split experiments across the whole population but a simple one can be to assign 20% (split into 10& and 10%) to the first experiment and assign a different 20% range to the second experiment.\n",
    "\n",
    "## Evaluation\n",
    "After the data have been collected we want to understand the differences caused by both treatments. In its most naive form we want to compare if the average purchase with a 5 point scale is \\$25.33 and with a 10 pointer it is \\$26.77 does that really mean that a 10 point scale is more profitable?\n",
    "\n",
    "Since we already discussed hypothesis testing we do not need to motivate the discussion too much. What we do need is a way to generate a distribution of those means to run hypothesis testing on. Two classic techniques that are very much valid for this type of analysis are [jackknife sampling](https://en.wikipedia.org/wiki/Jackknife_resampling) and [boostrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics%29).\n",
    "\n",
    "The essence of both is similar to K-fold cross-validation because we create multiple distributions by sampling from the distribution obtained from the experiment. If we use the whole population to compute an average we only have a point estimate, however, if we remove some of them and compute the average with the rest we could have a different average. If we do this repeatedly we will have a distribution of averages that can then be used for hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and reporting basics\n",
    "A data scientist should be able to communicate results to other people. The canonical way we do this is by creating visualizations of the data that are easy to understand. Most likely it is not within their responsibilities to create a graphical library to have three-dimensional plotting, but it probably is to be a power user of a reporting tool, or to be able to write SQL queries to create custom views of the data.\n",
    "\n",
    "This competency is much more related to the user experience that others have when accessing and understanding the reports created by the data scientist than it is to being able to create a deeply technical product. The utmost responsibility that we have is to surface useful data to the rest of an organization in a way that allows us to correct errors, be more efficient, have better strategic thinking, and so on.\n",
    "\n",
    "## Consumer types\n",
    "It is useful to think about the profile of people that will visualize the data. One potential classification is:\n",
    "\n",
    "- Casual users: These are users that probably do not want custom reports and only need to visualize data every now and then. They are probably contempt with having read-only dashboards that help them in their day-to-day work. People in the same role, who are casual users, probably have the same reporting needs.\n",
    "\n",
    "- Power users (the pivoters): One level above casual users we have what we can consider reporting power users. These are people who tend to be more tech savvy, they find standard reports useful but sometimes they need to download a copy of the data and do a pivot table manually because the current reports do not work for their needs. Some of them may even have some SQL knowledge to write very basic queries. The data scientists role is to expose as much data as needed so that the power users can do manipulation of the data on top.\n",
    "\n",
    "- Power coders: Probably developers who can probably extract any piece of information if given a direct connection to the data source. Arguably we do not need to worry about this type of user other than making sure there is standardization across the code created to access data directly and whatever scripts this role develops.\n",
    "\n",
    "Knowing what type of consumer we have is useful to define the granularity of the data exposed and the level of control we need to give them.\n",
    "\n",
    "## Tools\n",
    "The most basic reporting tool we have is `matplotlib`. However, this is probably only useful to programmers and the majority of consumers are probably not coders. Having said that, tools like the very one we are using right now, Jupyter, can help us use this library as a way of incorporating into a report.\n",
    "\n",
    "On top of this type of visualization library we tend to find full systems that we can use to make sense of the data through reporting. Some desirable characteristics of the tool are:\n",
    "\n",
    "- Easy to use\n",
    "- Allows the user to define custom views\n",
    "- It provides table reports with pivoting mechanisms\n",
    "- It can join multiple sources of data into a single report\n",
    "- Secure, both from the outside access point of view but also from protecting data on a need-to-know basis perspective.\n",
    "\n",
    "Remember that the choice of tool will directly impact all consumers of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration details\n",
    "To run the code samples, outside of the Jupyter environment setup, it is necessary for the person to install Python 3 locally and install the following modules (`pip` commands given for convenience):\n",
    "\n",
    "```bash\n",
    "pip3 install findspark\n",
    "\n",
    "pip3 install matplotlib\n",
    "pip3 install numpy\n",
    "pip3 install pandas\n",
    "pip3 install scipy\n",
    "pip3 install sklearn\n",
    "```\n",
    "\n",
    "During the workshop we also used [Spark](https://spark.apache.org/) and [PostgreSQL](https://www.postgresql.org/). Both are (mostly) well documented and the community is large enough that most common problems are crowd-solved in sites like StackOverflow, also."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "default_view": {},
   "name": "ejemplo_latex.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
