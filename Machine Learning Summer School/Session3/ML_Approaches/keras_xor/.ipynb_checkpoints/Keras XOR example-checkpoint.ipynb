{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVeMa 2018\n",
    "![title](evema-logo.jpeg)\n",
    "- Instructor: M.Sc. Blaž Meden, M.Sc. Žiga Emeršič\n",
    "\n",
    "- Authors: \n",
    "    - Saúl Calderón, Žiga Emeršič, Ángel García, Blaž Meden, Felipe Meza, Juan Esquivel, Martín Solís\n",
    "    - Mauro Méndez, Manuel Zumbado. \n",
    "    \n",
    "# Keras  XOR example \n",
    "\n",
    "## Understanding the model of our neural net\n",
    "Let’s take another look at our model from the previous article. We’ll be exploring what it actually means line by line.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "# the four different states of the XOR gate\n",
    "training_data = np.array([[0,0],[0,1],[1,0],[1,1]], \"float32\")\n",
    "\n",
    "# the four expected results in the same order\n",
    "target_data = np.array([[0],[1],[1],[0]], \"float32\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "model.fit(training_data, target_data, nb_epoch=500, verbose=2)\n",
    "\n",
    "print model.predict(training_data).round()\n",
    "```\n",
    "Let’s focus on the list of imports first. These are all things that we need to bring into scope because we need them in the rest of the code.\n",
    "```python\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "```\n",
    "\n",
    "The Python ecosystem has pretty strong math support. One of the most popular libraries is numpy which makes working with arrays a joy. Keras also uses numpy internally and expects numpy arrays as inputs. We import numpy and alias it as np which is pretty common thing to do when writing this kind of code.\n",
    "\n",
    "Keras offers two different APIs to construct a model: a functional and a sequential one. We’re using the sequential API hence the second import of Sequential from keras.models.\n",
    "\n",
    "Neural networks consist of different layers where input data flows through and gets transformed on its way. There are a bunch of different layer types available in Keras. These different types of layer help us to model individual kinds of neural nets for various machine learning tasks. In our specific case the Dense layer is what we want. We’ll explain it in more detail later on.\n",
    "\n",
    "What follows are two sets of data.\n",
    "```python\n",
    "# the four different states of the XOR gate\n",
    "training_data = np.array([[0,0],[0,1],[1,0],[1,1]], \"float32\")\n",
    "```\n",
    "```python\n",
    "# the four expected results in the same order\n",
    "target_data = np.array([[0],[1],[1],[0]], \"float32\")\n",
    "````\n",
    "\n",
    "\n",
    "````\n",
    "a -> having tired eyes\n",
    "b -> having sore throat\n",
    "c -> lublanas disease positive / negative (flapped ears syndrome)\n",
    "\n",
    "a | b | c = a ^ b\n",
    "--|---|------\n",
    "0 | 0 | 0\n",
    "0 | 1 | 1\n",
    "1 | 0 | 1\n",
    "1 | 1 | 0\n",
    "````\n",
    "We initialize training_data as a two-dimensional array (an array of arrays) where each of the inner arrays has exactly two items. As we’ve already described in the previous article, each of these pairs has a corresponding expected result. That’s why we could solve the whole task with a simple hash map but let’s carry on.\n",
    "\n",
    "We setup target_data as another two-dimensional array. All the inner arrays in target_data contain just a single item though. Each inner array of training_data relates to its counterpart in target_data. At least, that’s essentially what we want the neural net to learn over time. The value ```[0, 0]``` means 0, ```[0, 1]``` means 1 and so on and so forth.\n",
    "\n",
    "Let’s get to the most interesting part, the model!\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "The first line sets up an empty model using the Sequential API. But what’s going on in the second line?\n",
    "\n",
    "We’re adding a Dense layer to our model which we’ll explain in more detail a bit later. For now, let’s focus on its configuration. We set input_dim=2 because each of our input samples is an array of length 2 (```[0, 1], [1, 0]``` etc.). If we had input data such as ```[0, 1, 1]``` our input_dim would be 3. You get the idea.\n",
    "\n",
    "The more interesting question is: What does the 16 stand for? It’s the dimension of the output for this layer. If we think about our model in terms of neurons it means that we have two input neurons (```input_dim=2```) spreading into 16 neurons in a so called hidden layer.\n",
    "\n",
    "We also added another layer with an output dimension of 1 and without an explicit input dimension. In this case the input dimension is implicitly bound to be 16 since that’s the output dimension of the previous layer.\n",
    "\n",
    "We can visualize our model like this.\n",
    "\n",
    "![model](xor_model.png)\n",
    "\n",
    "## A neural network\n",
    "\n",
    "Let’s just take it as this for now. We’ll come back to look at what the number of neurons means in a moment.\n",
    "\n",
    "But there’s one more thing in the current snippet that may scare us: ```python activation='relu' ```. What does that mean? Do we finally have to talk about the scary math? Not really! Remember that neural networks are all about taking your inputs and transforming them into some output. Obviously there has to be some sort of math in between. There also need to be some moving parts, otherwise there wouldn’t be a chance for the model to learn anything. The moving parts are the so called weights and a simplified version of the math looks roughly like this.\n",
    "```python\n",
    "output = activation(input x weight)\n",
    "```\n",
    "By setting activation='relu' we specify that we want to use the relu function as the activation function. But really it doesn’t matter as much as you may think. We could totally change this to use the sigmoid function and it would still work. We could even use no activation function at all so that our algorithm essentially becomes.\n",
    "```python\n",
    "output = input x weight\n",
    "```\n",
    "Yes, that’s right. You can solve the XOR problem even without any activation function at all. It would just take much more time to train the model. Please don’t get us wrong. We aren’t saying the activation function doesn’t matter. In fact it matters a lot. But for our specific task which is very trivial, it matters less than people may think when they see the code for the very first time.\n",
    "\n",
    "There’s one last thing we have to do before we can start training our model. We have to configure the learning process by calling ```python model.compile(...)``` with a set of parameters.\n",
    "\n",
    "```python\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])\n",
    "```\n",
    "In order for the neural network to be able to make the right adjustments to the weights we need to be able to tell how good our model is performing. Or to be more specific, with neural nets we always want to calculate a number that tells us how bad our model performs and then try to get that number lower.\n",
    "\n",
    "That number is the so called loss and we can decide how the loss is calculated. Similar to how we picked relu as our activation function we picked mean_squared_error as our loss function simply because it’s a well proven loss function. We could change it to binary_crossentropy and our model would still continue to work. Again, we aren’t saying all loss functions can be used interchangeably. They do serve specific use cases. It’s just that we don’t have to understand all the heavy math behind each function to get going!\n",
    "\n",
    "That brings us to the next parameter, the optimizer. The job of the optimizer is it to find the right adjustments for the weights. I’m sure by now you may guess how we picked adam as our optimizer of choice. Right, because it’s a well proven one!\n",
    "\n",
    "The third parameter, metrics is actually much more interesting for our learning efforts. Here we can specify which metrics to collect during the training. We are interested in the binary_accuracy which gives us access to a number that tells us exactly how accurate our predictions are. More on that later.\n",
    "\n",
    "And that’s all we have to set up before we can start training our model. We kick off the training by calling model.fit(...) with a bunch of parameters.\n",
    "\n",
    "model.fit(training_data, target_data, nb_epoch=500, verbose=2)\n",
    "The first two params are training and target data, the third one is the number of epochs (learning iterations) and the last one tells keras how much info to print out during the training.\n",
    "\n",
    "Once the training phase finished we can start making predictions.\n",
    "\n",
    "print model.predict(training_data).round()\n",
    "Please note that in a real world scenario our predictions would be tested against data that the neural network hasn’t seen during the training. That’s because we usually want to see if our model generalizes well. In other words, does it work with new data or does it just memorize all the data and expected results it had seen in the training phase? However, with this toy task there are really only our four states and four expected outputs. No way to proof generalization here.\n",
    "\n",
    "Also note that we are rounding the output to get clear binary answers. Neural networks calculate probabilities. If we wouldn’t round we would see something like 0.9993... and 0.00034... instead of 1 and 0 which isn’t exactly what we want.\n",
    "\n",
    "## Finding the right model\n",
    "Now that we know what all those numbers mean. Let’s take a closer look at how we configured the layers. Why did we pick the Dense layer in the first place?\n",
    "\n",
    "The answer is simple: Because our input data is one-dimensional. Wait! Didn’t we just say we setup training_data as a two-dimensional array? Yes, that’s right. It’s an array of arrays but only because it’s an array holding different samples to feed into our network. Each sample though is one-dimensional. That is what counts for when we have to pick between different types of layers. Consider each sample would be an array or arrays instead.\n",
    "\n",
    "```python\n",
    "other_training_data = np.array([[[0,0],\n",
    "                                 [0,0]],\n",
    "                                [[1,0],\n",
    "                                 [0,1]]\n",
    "                                [[0,1],\n",
    "                                 [1,0]],\n",
    "                                [[1,1],\n",
    "                                 [1,1]]], \"float32\")\n",
    "```\n",
    "If that was the case we’d have to pick a different layer because a Dense layer is really only for one-dimensional input. We’ll get to the more advanced use cases with two-dimensional input data in another blog post soon.\n",
    "\n",
    "Now that we know we have to pick a Dense layer, how do we find the right configuration? We’ve already learned why we set the input_dim to 2 and that setting the output dimension to 16 means spreading into 16 neurons in a so called hidden layer. But why 16?\n",
    "\n",
    "The answer may surprise you but it’s simply a question of measuring the performance of our model and tweaking the size of the model accordingly. Remember that this number affects the number of weights that exists in our model. More weights mean more room for the model to care for certain features in the data we feed to it.\n",
    "\n",
    "Special Tip: As a rule of thumb the model should be big enough to deal with the task but not bigger. If the model is too big it may start finding pattern in your data that are actually irrelevant for the problem at hand. Keeping the model at a reasonable size means it’s forced to look at the relevant pattern.\n",
    "\n",
    "## Measure and tweak\n",
    "Let’s run our code and take a look at the output. If you aren’t sure how to run it please revisit out previous post. Alternatively jump to the end of this section to find the interactive embedded lab or open it on MachineLabs directly.\n",
    "\n",
    "```python\n",
    "Epoch 1/20000\n",
    "0s - loss: 0.7443 - binary_accuracy: 0.2500\n",
    "Epoch 2/20000\n",
    "0s - loss: 0.7428 - binary_accuracy: 0.2500\n",
    "...\n",
    "Epoch 62/20000\n",
    "0s - loss: 0.6881 - binary_accuracy: 0.2500\n",
    "Epoch 63/20000\n",
    "0s - loss: 0.6876 - binary_accuracy: 0.5000\n",
    "...\n",
    "0s - loss: 0.6801 - binary_accuracy: 0.5000\n",
    "Epoch 81/20000\n",
    "0s - loss: 0.6797 - binary_accuracy: 0.7500\n",
    "...\n",
    "Epoch 283/20000\n",
    "0s - loss: 0.5948 - binary_accuracy: 0.7500\n",
    "Epoch 284/20000\n",
    "0s - loss: 0.5943 - binary_accuracy: 1.0000\n",
    "```\n",
    "        \n",
    "The interesting number we want to focus on is binary_accuracy. Guess what the 0.2500 at the first two epochs mean? If you’re thinking it means that our model predicts one out of our four states correctly you’re damn right. It took us 63 epochs to predict half of the four states correctly. After 284 epochs the model makes perfect predictions for all of our four XOR states.\n",
    "\n",
    "Now we can start making changes to our model and see how it affects the performance. Let’s try to increase the size of our hidden layer from 16 to 32.\n",
    "```python\n",
    "0s - loss: 0.6023 - binary_accuracy: 0.7500\n",
    "Epoch 124/20000\n",
    "0s - loss: 0.6015 - binary_accuracy: 1.0000\n",
    "        ```\n",
    "Now, that’s cool! Took us only 124 epochs to get to an accuracy of 100%!\n",
    "\n",
    "What if we stack in another layer?\n",
    "```python\n",
    "model.add(Dense(32, input_dim=2, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "Interesting. That change brings us down to 56 epochs to solve the task. But would that be the same as just using one hidden layer with a size of 64?\n",
    "\n",
    "```python\n",
    "model.add(Dense(64, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "Turns out, it isn’t. With this configuration our model starts making perfect predictions after 32 epochs.\n",
    "\n",
    "Notice how we are able to play and figure out lots of interesting details once we start looking at the right metrics?\n",
    "\n",
    "Solving XOR with no activation function at all.\n",
    "Let’s see if we can hold our claim of solving XOR without any activation function at all. We change our model to look like this.\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=2, activation='linear'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "```\n",
    "Wait? But this says we actually do use an activation function called linear! What’s going on here?\n",
    "\n",
    "\n",
    "If we look at the keras source code we’ll see it is defined as this.\n",
    "\n",
    "```python\n",
    "def linear(x):\n",
    "    '''\n",
    "    The function returns the variable that is passed in, so all types work.\n",
    "    '''\n",
    "    return x\n",
    "```\n",
    "\n",
    "The function simply returns it’s input without applying any math, so it’s essentially the same as using no activation function at all.\n",
    "\n",
    "Honestly, this is just a little fun experiment but if we increase the number of epochs we can see that even with this config we can build up a net to make 100 % correct predictions.\n",
    "\n",
    "```python\n",
    "Epoch 3718/20000\n",
    "0s - loss: 0.2500 - binary_accuracy: 1.0000\n",
    "```\n",
    "You can play with all these different configurations and see there effect from right within this embedded lab.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "Whew! If you made it this far we’ll have to say THANK YOU for bearing so long with us just for the sake of understanding a model to solve XOR. If there’s just one take away we hope it’s that we don’t have to be a mathematician to start with machine learning.\n",
    "\n",
    "Once we understood some basics and learn how to measure the performance of our network we can figure out a lot of exciting things through trial and error.\n",
    "\n",
    "## Exercise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n"
     ]
    }
   ],
   "source": [
    "# the four different states of the XOR gate\n",
    "training_data = np.array([[0,0],[0,1],[1,0],[1,1]], \"float32\")\n",
    "print(training_data.shape)\n",
    "noise = (np.random.rand(training_data.shape[0], training_data.shape[1]) - 0.5) / 2.0\n",
    "training_data += noise\n",
    "\n",
    "# the four expected results in the same order\n",
    "target_data = np.array([[0],[1],[1],[0]], \"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 9s - loss: 0.2548 - binary_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.2542 - binary_accuracy: 0.5000\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.2536 - binary_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.2530 - binary_accuracy: 0.5000\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.2524 - binary_accuracy: 0.5000\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.2518 - binary_accuracy: 0.5000\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.2512 - binary_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.2506 - binary_accuracy: 0.5000\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.2501 - binary_accuracy: 0.7500\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.2497 - binary_accuracy: 0.7500\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.2492 - binary_accuracy: 0.7500\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.2488 - binary_accuracy: 0.7500\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.2483 - binary_accuracy: 0.7500\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.2478 - binary_accuracy: 0.7500\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.2473 - binary_accuracy: 0.7500\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.2469 - binary_accuracy: 0.7500\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.2464 - binary_accuracy: 0.7500\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.2459 - binary_accuracy: 0.7500\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.2454 - binary_accuracy: 0.7500\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.2450 - binary_accuracy: 0.7500\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.2445 - binary_accuracy: 0.7500\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.2440 - binary_accuracy: 0.7500\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.2436 - binary_accuracy: 0.7500\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.2431 - binary_accuracy: 0.7500\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.2426 - binary_accuracy: 0.7500\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.2421 - binary_accuracy: 0.7500\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.2416 - binary_accuracy: 0.7500\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.2412 - binary_accuracy: 0.7500\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.2407 - binary_accuracy: 0.7500\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.2402 - binary_accuracy: 0.7500\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.2397 - binary_accuracy: 0.7500\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.2392 - binary_accuracy: 0.7500\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.2388 - binary_accuracy: 0.7500\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.2383 - binary_accuracy: 0.7500\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.2378 - binary_accuracy: 0.7500\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.2373 - binary_accuracy: 0.7500\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.2368 - binary_accuracy: 0.7500\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.2364 - binary_accuracy: 0.7500\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.2359 - binary_accuracy: 0.7500\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.2354 - binary_accuracy: 0.7500\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.2349 - binary_accuracy: 0.7500\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.2344 - binary_accuracy: 0.7500\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.2340 - binary_accuracy: 0.7500\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.2335 - binary_accuracy: 0.7500\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.2330 - binary_accuracy: 0.7500\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.2326 - binary_accuracy: 0.7500\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.2321 - binary_accuracy: 0.7500\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.2316 - binary_accuracy: 0.7500\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.2311 - binary_accuracy: 0.7500\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.2306 - binary_accuracy: 0.7500\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.2302 - binary_accuracy: 0.7500\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.2297 - binary_accuracy: 0.7500\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.2293 - binary_accuracy: 0.7500\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.2288 - binary_accuracy: 0.7500\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.2283 - binary_accuracy: 0.7500\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.2279 - binary_accuracy: 0.7500\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.2274 - binary_accuracy: 0.7500\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.2270 - binary_accuracy: 0.7500\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.2265 - binary_accuracy: 0.7500\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.2261 - binary_accuracy: 0.7500\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.2256 - binary_accuracy: 0.7500\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.2252 - binary_accuracy: 0.7500\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.2247 - binary_accuracy: 0.7500\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.2243 - binary_accuracy: 0.7500\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.2239 - binary_accuracy: 0.7500\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.2234 - binary_accuracy: 0.7500\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.2230 - binary_accuracy: 0.7500\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.2225 - binary_accuracy: 0.7500\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.2221 - binary_accuracy: 0.7500\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.2217 - binary_accuracy: 0.7500\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.2212 - binary_accuracy: 0.7500\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.2208 - binary_accuracy: 0.7500\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.2204 - binary_accuracy: 0.7500\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.2200 - binary_accuracy: 0.7500\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.2195 - binary_accuracy: 0.7500\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.2191 - binary_accuracy: 0.7500\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.2187 - binary_accuracy: 0.7500\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.2182 - binary_accuracy: 0.7500\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.2178 - binary_accuracy: 0.7500\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.2175 - binary_accuracy: 0.7500\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.2171 - binary_accuracy: 0.7500\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.2166 - binary_accuracy: 0.7500\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.2163 - binary_accuracy: 0.7500\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.2159 - binary_accuracy: 0.7500\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.2155 - binary_accuracy: 0.7500\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.2151 - binary_accuracy: 0.7500\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.2147 - binary_accuracy: 0.7500\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.2143 - binary_accuracy: 0.7500\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.2139 - binary_accuracy: 0.7500\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.2135 - binary_accuracy: 0.7500\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.2131 - binary_accuracy: 0.7500\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.2127 - binary_accuracy: 0.7500\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.2123 - binary_accuracy: 0.7500\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.2119 - binary_accuracy: 0.7500\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.2116 - binary_accuracy: 0.7500\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.2112 - binary_accuracy: 0.7500\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.2108 - binary_accuracy: 0.7500\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.2104 - binary_accuracy: 0.7500\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.2100 - binary_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.2096 - binary_accuracy: 1.0000\n",
      "{'loss': [0.25477930903434753, 0.25417807698249817, 0.25357702374458313, 0.25299355387687683, 0.252404123544693, 0.25180813670158386, 0.2512153685092926, 0.2506304085254669, 0.25014546513557434, 0.24968700110912323, 0.24922235310077667, 0.24875251948833466, 0.24827870726585388, 0.24780669808387756, 0.2473302036523819, 0.24685291945934296, 0.24637478590011597, 0.24590207636356354, 0.24543645977973938, 0.2449689358472824, 0.24449896812438965, 0.24402616918087006, 0.24355530738830566, 0.243080735206604, 0.24260257184505463, 0.24212604761123657, 0.24164877831935883, 0.2411690652370453, 0.24068701267242432, 0.2402028739452362, 0.23972295224666595, 0.23924046754837036, 0.238755002617836, 0.23826688528060913, 0.23777776956558228, 0.2372911274433136, 0.2368057668209076, 0.23636256158351898, 0.23590807616710663, 0.2354339212179184, 0.23494166135787964, 0.23443418741226196, 0.2339668869972229, 0.23349876701831818, 0.23302699625492096, 0.23255415260791779, 0.23207858204841614, 0.23160146176815033, 0.23112395405769348, 0.23064285516738892, 0.23017531633377075, 0.22971907258033752, 0.22925691306591034, 0.22879831492900848, 0.2283366322517395, 0.2278749644756317, 0.22743338346481323, 0.22699475288391113, 0.2265322059392929, 0.22606608271598816, 0.22562207281589508, 0.2251746505498886, 0.22473804652690887, 0.22429880499839783, 0.2238544523715973, 0.22340670228004456, 0.22297927737236023, 0.22254323959350586, 0.2220926284790039, 0.22166599333286285, 0.22124023735523224, 0.2208169549703598, 0.22039416432380676, 0.21996499598026276, 0.2195291519165039, 0.21909597516059875, 0.21865952014923096, 0.21822679042816162, 0.21784721314907074, 0.21745698153972626, 0.2170562893152237, 0.21663784980773926, 0.21625319123268127, 0.2158742994070053, 0.2154877483844757, 0.21509376168251038, 0.21469435095787048, 0.21430671215057373, 0.2139107584953308, 0.2135026901960373, 0.2130974531173706, 0.21269936859607697, 0.21233439445495605, 0.21194590628147125, 0.21156558394432068, 0.21118207275867462, 0.2107885777950287, 0.21039406955242157, 0.2099945843219757, 0.20959065854549408], 'binary_accuracy': [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "history = model.fit(training_data, target_data, epochs=100, verbose=2)\n",
    "\n",
    "model.save_weights(\"saved_weights.h5\")\n",
    "\n",
    "#print(history.history)\n",
    "\n",
    "loss = history.history[\"loss\"]\n",
    "acc = history.history[\"binary_accuracy\"]\n",
    "x = range(len(acc))\n",
    "\n",
    "plt.plot(x, loss, 'r')\n",
    "plt.show()\n",
    "plt.plot(x, acc, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_weights(\"saved_weights.h5\")\n",
    "\n",
    "print(model.predict(training_data).round())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Authors: *Saul Calderon, Angel García, Blaz Meden, Felipe Meza, Juan Esquivel, Martín Solís, Ziga Emersic, Mauro Mendez, Manuel Zumbado*\n",
    "\n",
    "References: \n",
    " - https://blog.thoughtram.io/machine-learning/2016/11/02/understanding-XOR-with-keras-and-tensorlow.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
