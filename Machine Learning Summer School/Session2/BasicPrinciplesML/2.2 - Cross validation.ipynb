{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVeMa 2018\n",
    "\n",
    "![logo](assets/logo.jpg \"Logo\")\n",
    "\n",
    "- Instructor: Žiga Emeršič.\n",
    "\n",
    "- Authors: \n",
    "    - Saúl Calderón, Martín Solís, Ángel García, Blaž Meden, Felipe Meza, Juan Esquivel\n",
    "    - Mauro Méndez, Manuel Zumbado. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "The most straight forward approach to model training and evaluation is to split the data into two parts. However, the data could contain some unwanted properties or relations in one of the parts.\n",
    "\n",
    "This can mean that our training will be unsuccessul or that we will erroneously evalute the predicition. For example, imagine that test data contains only the simple possible cases. The calculated prediction performance will therefore be misleadingly high.\n",
    "\n",
    "To counter that to some extent we always need to split data randomly (e.g. we permute data prior splitting it). However, there still is no ensurance, that some unwanted relations will not appear, especially when we are dealing with small amounts of data.\n",
    "\n",
    "Would not be great if we could train and test our model od the whole dataset? This is where cross validation comes in. We split the data into equal parts and then loop through all the combinations.\n",
    "\n",
    "There are many different types of cross validation. For the scipy (e.g. from sklearn.model_selection import StratifiedKFold) check http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection\n",
    "\n",
    "Stratified k-fold cross-validation, in which the folds are selected so that each fold contains roughly the same proportions of class labels.\n",
    "\n",
    "In repeated k-fold cross-validation the devisions are repeated n-times.\n",
    "\n",
    "With the cross validation we weigh between bias vs variance. If we do not perform cross validation at all, there will, of course, be no variance, but the bias could be high. If we on the other hand, split data into as many parts as there are samples we will decrease the bias, but increase the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Folds Cross Validation\n",
    "In K-Folds Cross Validation we split our data into k different subsets (or folds). We use k-1 subsets to train our data and leave the last subset (or the last fold) as test data. We then average the model against each of the folds and then finalize our model. After that we test it against the test set.\n",
    "\n",
    "How many folds? Well, the more folds we have, we will be reducing the error due the bias but increasing the error due to variance; the computational price would go up too, obviously — the more folds you have, the longer it would take to compute it and you would need more memory. With a lower number of folds, we’re reducing the error due to variance, but the error due to bias would be bigger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris.data.shape, iris.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What we would usually do ...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9705882352941176"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     iris.data, iris.target, test_size=0.45, random_state=0)\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "X_test.shape, y_test.shape\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this really OK? What if we would just by chance perform a very bad split? E.g. trivial samples in train set and very difficult ones in the test set - or vice versa.\n",
    "\n",
    "Possible solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96666667 1.         0.96666667 0.96666667 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n",
    "print(scores)\n",
    "#print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Out Cross Validation\n",
    "\n",
    "This is another method for cross validation, Leave One Out Cross Validation (by the way, these methods are not the only two, there are a bunch of other methods for cross validation). In this type of cross validation, the number of folds (subsets) equals to the number of observations we have in the dataset. Each sample is used once as a test set (singleton) while the remaining samples form the training set.\n",
    "\n",
    "This method is very computationally expensive and should be used on small datasets. If we have large amounts of data, use k-fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there anything else?\n",
    "\n",
    "If we do not have enough data or have a very variable model that introduces too much variance intou our experiments, we can use Bootstrap (also known as Bagging). Here we repeat some samples and also introduce new samples by averaging some of the existing samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: *Saul Calderon, Angel García, Blaz Meden, Felipe Meza, Juan Esquivel, Martín Solís, Ziga Emersic, Mauro Mendez, Manuel Zumbado*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
