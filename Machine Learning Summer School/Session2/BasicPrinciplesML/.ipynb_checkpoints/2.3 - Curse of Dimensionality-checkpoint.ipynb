{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVeMa 2018\n",
    "\n",
    "![logo](assets/logo.jpg \"Logo\")\n",
    "\n",
    "- Instructor: Žiga Emeršič.\n",
    "\n",
    "- Authors: \n",
    "    - Saúl Calderón, Martín Solís, Ángel García, Blaž Meden, Felipe Meza, Juan Esquivel\n",
    "    - Mauro Méndez, Manuel Zumbado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.freecodecamp.org/the-curse-of-dimensionality-how-we-can-save-big-data-from-itself-d9fa0f872335\n",
    "\n",
    "Also go through the approaches for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of model training is to prepare a model in such a way that it will be able to predict class affiliations (or some continues data) based on a set of attribute values.\n",
    "\n",
    "Let us define a number of attributes as the width, and the number of samples as the length of the data.\n",
    "\n",
    "For example: iris dataset has 4 attributes and 150 samples. Is that \"wide\" or \"narrow\"? What about Titanic (2201 samples with 3 attributes)? Or some medical data of cancer patients (30 patients, 20 attributes)? GEO GDS2191 has 24 samples and 14,903 attributes!\n",
    "\n",
    "We like \"thin\" data! Fat data NO NO!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading and Selection of Attributes\n",
    "\n",
    "When observing a set of $n$ attributes not all attributes are equally important and not all are independent. The independent attributes could be joint together or some attributes can even be removed. For example in the data of patients, the color of their hair is not important for cancer prediction. Wheras, the weight of a patient in kg and weight in pounds is completely correlated and should be joined (one removed).\n",
    "\n",
    "We can grade attributes using:\n",
    "* Filter feature selection: this approaches apply a statistical measure to assign a scoring to each feature. The features are ranked by the score and either selected to be kept or removed from the dataset.\n",
    "* Wrapper feature selection: the selection of a set of features is presented as a search problem, where different combinations are prepared, evaluated and compared to other combinations.\n",
    "* Embedded methods learn which features best contribute to the accuracy of the model while the model is being created. The most common type of embedded feature selection methods are regularization methods.\n",
    "\n",
    "See the board for the attribute explosion.\n",
    "\n",
    "For reduction of dimensionality we can use (transformation of the feature space):\n",
    "* Linear discriminant analysis (LDA)\n",
    "* Principal component analysis (PCA)\n",
    "* Non-negative matrix factorization (NMF)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: *Saul Calderon, Angel García, Blaz Meden, Felipe Meza, Juan Esquivel, Martín Solís, Ziga Emersic, Mauro Mendez, Manuel Zumbado*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
