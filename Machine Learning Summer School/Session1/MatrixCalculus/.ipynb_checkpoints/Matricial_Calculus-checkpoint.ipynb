{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matricial Calculus\n",
    "\n",
    "Below are presented basic concepts of matrix calculus, which consists of extending the concepts for differential and integral calculus in spaces of greater dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient \n",
    "\n",
    "Suppose a multivariate function that takes multiple inputs (represented in matrix $ A\\in\\mathbb{R}^{m\\times n}$) and returns a scalar output $s\\in\\mathbb{R}$, so $f:\\mathbb{R}^{m\\times n}\\rightarrow\\mathbb{R}$.\n",
    "\n",
    "The gradient of the function $f$ with respect to its input  $A\\in\\mathbb{R}^{m\\times n}$ is the matrix of partial derivatives defined as:\n",
    "\n",
    "$$\\nabla_{A}f\\left(A\\right)\\in\\mathbb{R}^{m\\times n}=\\begin{bmatrix}\\frac{\\partial f\\left(A\\right)}{\\partial A_{1,1}} & \\frac{\\partial f\\left(A\\right)}{\\partial A_{1,2}} & \\ldots & \\frac{\\partial f\\left(A\\right)}{\\partial A_{1,n}}\\\\\n",
    "\\frac{\\partial f\\left(A\\right)}{\\partial A_{2,1}} & \\frac{\\partial f\\left(A\\right)}{\\partial A_{2,2}} & \\ldots & \\frac{\\partial f\\left(A\\right)}{\\partial A_{2,n}}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial f\\left(A\\right)}{\\partial A_{m,1}} & \\frac{\\partial f\\left(A\\right)}{\\partial A_{m,2}} & \\ldots & \\frac{\\partial f\\left(A\\right)}{\\partial A_{m,n}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "in compact notation, each entry is given by:\n",
    "\n",
    "$$\\left(\\nabla_{A}f\\left(A\\right)\\right)_{i,j}=\\frac{\\partial f\\left(A\\right)}{\\partial A_{i,j}}$$\n",
    "\n",
    "in particular, for an entry defined in a vector $\\vec{x}\\in\\mathbb{R}^{n}$ the gradient is defined as:\n",
    "\n",
    "$$\\nabla_{\\vec{x}}f\\left(\\vec{x}\\right)=\\begin{bmatrix}\\frac{\\partial f\\left(A\\right)}{\\partial x_{1}}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial f\\left(A\\right)}{\\partial x_{n}}\n",
    "\\end{bmatrix}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Excercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the gradient **is only defined if the function returns a scalar**. This means that for example, it is not possible to take the gradient of $A\\,\\vec{x}$, since the result of such matrix product is a vector, and not a scalar.\n",
    "\n",
    "The partial matrix derivative is also a linear operator, such as the partial derivative of a multivariable function, so that it then satisfies the properties of homogeneity and superposition:\n",
    "\n",
    "* $\\nabla_{\\vec{x}}\\left(f\\left(\\vec{x}\\right)+g\\left(\\vec{x}\\right)\\right)=\\nabla_{\\vec{x}}f\\left(\\vec{x}\\right)+\\nabla_{\\vec{x}}g\\left(\\vec{x}\\right)$\n",
    "\n",
    "* For a scalar $s\\in\\mathbb{R}, \\nabla_{\\vec{x}}\\left(s\\,f\\left(\\vec{x}\\right)\\right)=s\\,\\nabla_{\\vec{x}}f\\left(\\vec{x}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Excercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a multidimensional function with an input vector is the function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$\n",
    "\n",
    "$$f\\left(\\vec{z}\\right)=\\vec{z}^{T}\\vec{z}=\\sum_{i=1}^{n}z_{i}^{2}$$\n",
    "\n",
    "which, as observed, calculates the product point $\\vec{z}\\cdot\\vec{z}$ of its input vector $\\vec{z}=\\begin{bmatrix}z_{1}\\\\\n",
    "\\vdots\\\\\n",
    "z_{m}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Examining each of the $m$ partial derivatives $\\frac{\\partial f\\left(\\vec{z}\\right)}{\\partial z_{k}}$ (you can ignore the fact that the input is given by a vector and treat like any multivariable function) you have:\n",
    "\n",
    "$\\require{cancel}$\n",
    "\n",
    "$$\\frac{\\partial f\\left(\\vec{z}\\right)}{\\partial z_{k}}=\\cancelto{0}{\\frac{\\partial}{\\partial z_{k}}z_{1}^{2}}+\\cancelto{0}{\\frac{\\partial}{\\partial z_{k}}z_{2}^{2}}+\\ldots+\\cancelto{2\\,z_{k}}{\\frac{\\partial}{\\partial z_{k}}z_{k}^{2}}+\\ldots+\\cancelto{0}{\\frac{\\partial}{\\partial z_{i}}z_{n}^{2}}=2\\,z_{k}.$$\n",
    "\n",
    "\n",
    "Thus the gradient vector is given by:\n",
    "\n",
    "$$\\nabla_{\\vec{z}}f\\left(\\vec{z}\\right)=\\begin{bmatrix}\\frac{\\partial f\\left(\\vec{z}\\right)}{\\partial z_{1}}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial f\\left(\\vec{z}\\right)}{\\partial z_{n}}\n",
    "\\end{bmatrix}=\\begin{bmatrix}2\\,z_{1}\\\\\n",
    "\\vdots\\\\\n",
    "2\\,z_{n}\n",
    "\\end{bmatrix}=2\\,\\vec{z}.$$\n",
    "\n",
    "and so then the equivalent of the derivative of a quadratic function of a variable is:\n",
    "\n",
    "$$\\nabla_{\\vec{z}}f\\left(\\vec{z}\\right)=\\nabla_{\\vec{z}}\\left(\\vec{z}^{T}\\vec{z}\\right)=2\\,\\vec{z}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "fig2 = plt.figure()\n",
    "fig3 = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax2 = fig2.gca(projection='3d')\n",
    "ax3 = fig3.gca(projection='3d')\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(0, 2, 0.01)\n",
    "Y = np.arange(0, 2, 0.01)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "#dot product for 2d vector\n",
    "Z = np.multiply(X,X) + np.multiply(Y,Y);\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(X, Y, Z)\n",
    "# Customize the z axis.\n",
    "ax.set_zlim(0, 5)\n",
    "\n",
    "#analytical gradient\n",
    "Xag = 2 * X;\n",
    "Yag = 2 * Y;\n",
    "\n",
    "\n",
    "#numerical gradient\n",
    "h = 0.01 #dx step\n",
    "gy, gx = np.gradient(Z, h)\n",
    "\n",
    "# Plot the surface.\n",
    "surf2 = ax2.plot_surface(X, Y, Xag)\n",
    "# Customize the z axis.\n",
    "ax2.set_zlim(0, 5)\n",
    "\n",
    "# Plot the surface.\n",
    "surf3 = ax3.plot_surface(X, Y, gx)\n",
    "# Customize the z axis.\n",
    "ax3.set_zlim(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if the input of the function is multiplied by a matrix $A\\in\\mathbb{R}^{m\\times n}$, so that the gradient $\\nabla f\\left(A\\,\\vec{x}\\right)$, with $\\vec{x}\\in\\mathbb{R}^{n}$? \n",
    "\n",
    "The gradient of $f$ must be interpreted as the evaluation of it at the point $A\\,\\vec{x}=\\vec{z}$, so the gradient is given by:\n",
    "$$\\nabla f\\left(A\\,\\vec{x}\\right)=\\nabla\\left(\\left(A\\,\\vec{x}\\right)^{T}\\left(A\\,\\vec{x}\\right)\\right)=2\\left(A\\,\\vec{x}\\right)=2\\,A\\,\\vec{x}\\in\\mathbb{R}^{m}$$\n",
    "\n",
    "Generalizing the previous function, which receives a vector $\\vec{x}\\in\\mathbb{R}^{n}$  as input, and with a known vector $\\vec{b}\\in\\mathbb{R}^{n}$:\n",
    "\n",
    "$$f\\left(\\vec{x}\\right)=\\vec{b}^{T}\\vec{x}=\\sum_{i=1}^{n}b_{i}x_{i}$$\n",
    "\n",
    "with its partial derivative is then given by:\n",
    "\n",
    "$$\\frac{\\partial f\\left(\\vec{x}\\right)}{\\partial x_{k}}=\\frac{\\partial}{\\partial x_{k}}\\sum_{i=1}^{n}b_{i}x_{i}=b_{k} \\enspace .$$\n",
    "\n",
    "And that's why we have:\n",
    "$$\\nabla_{\\vec{x}}\\left(\\vec{b}^{T}\\vec{x}\\right)=\\vec{b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Excercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the quadratic function (which, as already seen, results in a scalar):\n",
    "\n",
    "$$f\\left(\\vec{x}\\right)=\\vec{x}^{T}\\,A\\,\\vec{x}=\\vec{x}^{T}\\,\\begin{bmatrix}- & \\vec{a}_{1,:}^{T} & -\\\\\n",
    "- & \\vec{a}_{2,:}^{T} & -\\\\\n",
    " & \\vdots\\\\\n",
    "- & \\vec{a}_{m,:}^{T} & -\n",
    "\\end{bmatrix}\\,\\begin{bmatrix}x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "\\vdots\\\\\n",
    "x_{n}\n",
    "\\end{bmatrix}=\\begin{bmatrix}x_{1} & \\ldots & x_{n}\\end{bmatrix}\\,\\begin{bmatrix}\\vec{a}_{1,:}^{T}\\:\\vec{x}\\\\\n",
    "\\vec{a}_{2,:}^{T}\\:\\vec{x}\\\\\n",
    "\\vdots\\\\\n",
    "\\vec{a}_{m,:}^{T}\\:\\vec{x}\n",
    "\\end{bmatrix}=\\sum_{i=1}^{n}\\sum_{j=1}^{n}A_{i,j}x_{i}x_{j}$$\n",
    "\n",
    "To calculate the partial derivative $\\frac{\\partial f\\left(\\vec{x}\\right)}{\\partial x_{k}}$ for each component $x_{k}$ of the input vector $\\vec{x}$, decompose the nested summations in the cases in which the row and column of such summation is different from $k$, in which the row is equal to $k$, in addition to the case in which the column is equal to $k$, and finally, when it is in the row and column $k$:\n",
    "\n",
    "$$\\frac{\\partial f\\left(\\vec{x}\\right)}{\\partial x_{k}}=\\frac{\\partial}{\\partial x_{k}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}A_{i,j}x_{i}x_{j}$$\n",
    "\n",
    "$$\\Rightarrow\\frac{\\partial f\\left(\\vec{x}\\right)}{\\partial x_{k}}=\\frac{\\partial}{\\partial x_{k}}\\left[\\sum_{i\\neq k}^{n}\\sum_{j\\neq k}^{n}A_{i,j}x_{i}x_{j}+\\sum_{i\\neq k}^{n}A_{i,k}x_{i}x_{k}+\\sum_{j\\neq k}^{n}A_{k,j}x_{k}x_{j}+A_{k,k}x_{k}^{2}\\right]$$\n",
    "\n",
    "$$\\Rightarrow\\frac{\\partial f\\left(\\vec{x}\\right)}{\\partial x_{k}}=\\sum_{i\\neq k}^{n}A_{i,k}x_{i}+\\sum_{j\\neq k}^{n}A_{k,j}x_{j}+2A_{k,k}x_{k}=\\sum_{i=1}^{n}A_{i,k}x_{i}+\\sum_{j=1}^{n}A_{k,j}x_{j}$$\n",
    "\n",
    "Since it is assumed that in the quadratic form $A$ is symmetric, which means that $A=A^{T}\\Rightarrow A_{i,j}=A_{j,i}$, we have:\n",
    "\n",
    "$$\\Rightarrow\\frac{\\partial f\\left(\\vec{x}\\right)}{\\partial x_{k}}=\\sum_{i=1}^{n}A_{i,k}x_{i}+\\sum_{j=1}^{n}A_{k,j}x_{j}=2\\sum_{i=1}^{n}A_{k,i}x_{i}.$$\n",
    "\n",
    "That is why it is concluded that the gradient of the quadratic form is given by:\n",
    "\n",
    "$$\\nabla_{\\vec{x}}\\left(\\vec{x}^{T}\\,A\\,\\vec{x}\\right)=2\\,A\\,\\vec{x}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Excercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following matrix derivatives are then concluded:\n",
    "\n",
    "* $\\nabla\\left(\\vec{x}^{T}\\vec{x}\\right)=2\\,\\vec{x}$\n",
    "\n",
    "* $\\nabla\\left(\\left(A\\,\\vec{x}\\right)^{T}\\left(A\\,\\vec{x}\\right)\\right)=2\\,A\\,\\vec{x}$\n",
    "\n",
    "* $\\nabla_{\\vec{x}}\\left(\\vec{b}^{T}\\vec{x}\\right)=\\vec{b}$\n",
    "\n",
    "* $\\nabla_{\\vec{x}}\\left(\\vec{x}^{T}\\,A\\,\\vec{x}\\right)=2\\,A\\,\\vec{x}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
